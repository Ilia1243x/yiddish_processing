В данном разделе находятся два .py файла, несколько .txt файлов и .json файлов (К сожалению гитхаб не позволяет залить слишком большие файлы, так что самих двух папок с текстами и большого json файла тут нет)

Главный файл - lemmatizer.py Это лемматизатор текстов, лемматизатор рукописный (потому что для Идиша их пока что нет существующих... Но в переписке с создателем сайта https://www.yiddishbookcenter.org/collections/digital-yiddish-library я получил от него информацию, что он планирует в будущем написать нормальный лемматизатор с помощью методов машинного обучения), в нем записаны чуть ли не все правила языка Идиш, которые можно найти в открытом доступе. Однако, Идиш слишком сложный язык, и этого не хватает в ряде случаев. Поэтому приходится использовать статистику за неимением лучших способов. Также лемматизатор по максимуму использует словари, которые можно достать из интернета. 

adjectives.txt - список популярный прилагательных
adverbs.txt - список популярных наречий
female_nouns.txt - список популярных существительных женского рода 
male_nouns.txt - список популярных существительных мужского рода
neuter_nouns.txt - список популярных существительных среднего рода
nouns.txt - список популярных существительных без разметки по родам
verbs.txt - список популярных глаголов

В коде lemmatizer.py также есть списки предлогов, союзов и т.д. Но поскольку их не так много, и они сами по себе короче, то я прямо создал множества в коде, а не файлы как вот выше.

data.json - словарь неправильных глаголов, собранных по всему интернету (инфинитив:прошедшее время)
forms.json - словарь форм, который я использовал для статистического анализа

Код make_statistic.py как раз таки создает forms.json файл. Он очень похож на файл lemmatizer.py но в целом заглядывать в него и запускать его особо не надо, добавил для возможности восстановления forms.json в случае чего

В коде lemmatizer.py я писал максимально много комментариев для понимания происходящего, местами код не структурирован (в начале несколько сот строк для извлечения файлов, составления словарей, перевода букв в единый стиль, перевода в нужный вид и т.д.). 

Еще в разделе есть две папки:

after - папка с текстами после обработки Тессерактом и фиксом расстоянием Левенштейна, но до лемматизации
res - конечный результат: папка с текстами после лемматизации (я также попробовал убрать несколько инностранных текстов, так что она чуть поменьше)

Также в раздел были добавлены код скрипта (yid_scraping.ipynb), который был использован для скачивания книг с вышеупомянутого сайта, код lev.py для подсчета расстояния Левенштейна и чистки и код multi_transform.py, немного измененная версия которого запускалась на суперкомпьютере для перевода pdf в txt.
